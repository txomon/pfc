\chapter{Arquitectura general}

En esta sección se va a describir cómo están implementadas las partes del kernel de Linux que nos interesan, véase
\begin{itemize}
\item Gestión de las interfaces de red
\item Gestión de las interrupciones de las interfaces de red
\item Recepción de paquetes de la red
\item Procesado de paquetes de la red
\item Creación de ficheros virtuales seq\_file
\end{itemize}

Las explicaciones van a efectuarse sobre el conocimiento que se puede adquirir leyendo los libros de programación en entorno de kernel, \cite{OLDD09}, \cite{OULN05} y \cite{OUTL00}. Aunque habrá veces en las que se referencie alguno de los anteriores libros, en general, todas las explicaciones se van a hacer aquí porque no hay ningún tipo de documentación en la que apoyarse.

\section{Gestión de las interfaces físicas de red}

Para entender como se ha hecho el diseño de bajo nivel de una parte de las cosas, es necesario entender como está montado el subsistema de red, de gestión de recursos, y gestión de dispositivos físicos. Para ello, en \cite{OLDD09} se explica como se gestionan los dispositivos.

En el sistema, se gestionan todos los dispositivos con \code{struct device}, esta estructura es una estructura genérica para la gestión de dispositivos. Los dispositivos de red se gestionan con una estructura más específica, \code{struct net\_device} que se cambiará en futuras versiones del código para eliminar la mezcla de alto y bajo nivel de gestión.

Otra estructura que ha aparecido, es \code{struct napi\_struct}, esta estructura, asociada a un dispositivo de red \code{struct net\_device}, contiene información sobre cual es la función que hay que llamar para hacer poll\footnote{Hacer poll se refiere en este caso a la atención de las necesidades de la tarjeta de red bajo demanda. En un estudio reciente del código se ha encontrado que no solo se cumplen funciones de recepción sino también de transmisión.} o cual es el número máximo de paquetes que puede capturar de una sola vez, \code{weight}, y más datos que se irán introduciendo según sea necesario.

Por el momento, basta explicar que una interfaz, se registra en el sistema (instanciando una estructura \code{struct net\_device}), y al hacerlo, registra consigo todas las colas napi de las que dispone.

La interfaz la registra el driver en su carga, esto es, el driver, creado como un módulo de kernel, es insertado dentro del sistema cuando este ve que tiene una interfaz que es soportada por el driver. En ese momento, en la inicialización del driver, este mira que tipo de interfaz o interfaces se están registrando e inicializa el puntero a función de poll mencionado antes dentro de \code{struct napi\_struct} dependiendo de que tipo de gestión requiera.

De esta manera, si una interfaz de red tiene un tipo de interrupción asociado, y otra, por ejemplo soporta msix, se utilizará una función de poll diferente para cada una.

\section{Gestión de las interrupciones de las interfaces de red}
Una vez se ha registrado la función de poll que corresponde al interfaz, la rutina de interrupción hardware de la misma (esto se hace en la gestión de las interrupciones del dispositivo normal, \code{struct device}), así como muchos otros detalles que no nos interesan en la realización de este proyecto, la interfaz está lista para funcionar. A continuación se explica este proceso más detalladamente en las partes que competen.

\subsection{Inicialización del hardware}
Una vez se han inicializado los campos genéricos de \code{struct net\_device}, se inicializan los campos específicos en caso de tratarse de un dispositivo Ethernet, en el fichero \code{/net/ethernet/eth.c}, aunque a través de varias macros y defines, específicamente, se suele utilizar la función \code{ether\_setup()}. Por lo tanto, una vez se tiene el dispositivo registrado, con todo lo que ello conlleva, se activa el dispositivo, y el sistema tiene en cuenta las interrupciones hardware del mismo.

\subsection{Rutina de servicio a la interrupción hardware: hardirq}
En el momento en que llega una interrupción, al sistema, se ejecuta la función de hardirq del driver específico al que esté asignada esa interrupción. En el caso de las interrupciones hardware, éstas deben devolver un valor de la estructura \code{irqreturn\_t}, \code{IRQ\_NONE}, \code{IRQ\_HANDLED} o \code{IRQ\_WAKE\_THREAD}.

En el caso específico de las interrupciones de red, estas rutinas contienen siempre una llamada a \code{napi\_schedule()}. Es importante tener en cuenta que en el caso de existir una función con dos barras antes de ella, por ejemplo, \code{\_\_napi\_schedule()}, esta significa que hay algún tipo de comprobación extra, en este caso, la llamada a \code{napi\_schedule()} se encarga de asegurarse de que no haya una rutina softirq corriendo antes de intentar planificarla y llamar a \code{\_\_napi\_schedule()}, ésta se encarga de deshabilitar las interrupciones y llamar a \code{\_\_\_\_napi\_schedule()}, que se encarga propiamente de planificar la rutina de softirq.

Por lo tanto, desde la hardirq, se llama a \code{napi\_schedule()}, ya que ésta se encarga de planificar la interrupción si es necesaria. En algunos casos, como en el driver e1000e de Intel, se quiere hacer una inicialización de algunas variables en caso de que se planifique, él se encarga de comprobar si hay una softirq en marcha, y llama directamente a \code{\_\_napi\_schedule()}.

\subsection{Rutina de servicio a la interrupción software: softirq}

Esta rutina es una interrupción software, esto significa que es una interrupción en el flujo de las aplicaciones, pero síncrona, en la que el planificador del sistema puede decidir cuando ejecutar, aunque con una prioridad mayor que la de los procesos normales del kernel.

La softirq, llamada \code{net\_rx\_action()} en el fichero \code{/net/core/dev.c}, se encarga principalmente de llamar a las funciones de poll, controlando el tiempo que se pasa haciéndolo y el número de paquetes que se captura durante su ejecución. Hay una variable, \code{netdev\_budget}, definida en \code{/net/core/dev.c} que contiene el número máximo de paquetes que deben ser capturados en una sola softirq. Esta variable tiene el flag \code{\_\_read\_mostly} que básicamente significa que todos los procesadores la tendrán en su caché para acceso más inmediato, tal y como está descrito en \cite{RERI11}.

Por lo tanto, una softirq se ve limitada al valor límite de \code{netdev\_budget} paquetes y a un valor estático de tiempo de ejecución (2 jiffies). La rutina se encarga de hacer poll a las interfaces que tiene apuntadas en la lista de polling, en \code{poll\_list} y finaliza cuando se ha sobrepasado alguno de los límites anteriormente expuestos o cuando se acaban las interfaces a las que hacer poll.

La función de poll recibe dos parámetros, la estructura \code{struct napi\_struct} que describe la cola de la que se tienen que extraer paquetes, y el número de paquetes total que está autorizado a extraer. Para llevar un control exacto de los paquetes, esta devuelve el número de paquetes total recibido, llamado \code{work}.

\section{Recepción de paquetes de red}

Aunque la función de poll se encarga de entregar al sistema los paquetes de red, estos han sido capturados por la tarjeta sin ninguna intervención. Por lo tanto, se puede suponer que los paquetes son entregados al sistema en tandas de work paquetes.

La función de poll, al contrario de lo que se pueda pensar y aún siendo llamada en la softirq que está pensada para atender la captura de paquetes, en muchas ocasiones se utiliza para atender primero el envío de paquetes.

Con envío de paquetes, nos podemos referir tanto a la liberación de memoria tras el envío de paquetes, como al envío real de paquetes desde la interfaz. Esto se da en los 2 drivers principales, el driver tg3 de broadcom y el e1000e de intel. Tanto como esto podría ser un retardo leve en la realización de la captura, hay casos en los que en caso de haber más tarjetas, esta característica puede afectar negativamente al rendimiento.

Suponiendo que no hay ningún fallo en la retransmisión, se recibirán todos los paquetes posibles, hasta llegar al cupo del \code{budget}. Estos paquetes se van procesando de uno en uno, y se mandan a la función \code{napi\_gro\_receive()}.

\section{Procesado de los paquetes de red}

La función \code{napi\_gro\_receive()} es la que se usa en caso de que el dispositivo hardware esté utilizando la lógica NAPI. En nuestro caso, solo nos interesa esta lógica, pero se mencionará cual era brevemente el anterior método de funcionamiento, ya que NAPI se apoya sobre él.

Por lo tanto, \code{napi\_gro\_receive()} se encarga de la recepción de paquetes obtenidos a través de NAPI. Esta función es la mencionada en \cite{DAVE10}, y se encarga de hacer super-paquetes que más tarde serán procesados como uno solo. Una vez se ha gestionado adecuadamente el paquete y se ha ensamblado dentro de un super paquete, este pasa a ser capturado utilizando la función \code{netif\_receive\_skb()}.

Esta función es la que básicamente se encarga de pasar hacia el protocolo de nivel superior el paquete, está descrita como la función principal de recepción de paquetes. En la actualidad casi todos los paquetes de red se analizan en bloques.

Uno de los mayores cambios, y por lo que se va a tener dificultades de diseño en el siguiente capítulo es porque la interfaz GRO está implantada, y habrá que hacer que ksensor funcione con super paquetes. Esto, a su vez, planteará problemas en las pruebas, ya que los paquetes que se capturen (antes de hacer su fusión) y los paquetes analizados (después de hacer su fusión) darán diferentes números.

De todos modos, este y otros problemas se explicarán en sus secciones correspondientes.