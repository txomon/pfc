% !TeX spellcheck = es_ES
% !TeX root = main.tex

\chapter{Introducción}

En los últimos años, las tecnologías informáticas de la comunicación están revolucionando la manera de comunicarse del mundo entero. Tener acceso a Internet se ha convertido en algo necesario para poder ponerse en contacto con el resto del mundo.

Cada vez hay más dispositivos conectados, con el consiguiente crecimiento del tráfico en todas las redes. Incluso empresas que antes contrataban redes privadas para sus comunicaciones, ahora utilizan Internet para interconectar sus redes.

Como en cualquier servicio crítico el mantenimiento debe ser proactivo y por ello se han creado métodos para garantizar la máxima eficiencia de las comunicaciones. Para ello se debe garantizar la seguridad en la red y la detección instantánea de problemas, asegurando así la \textit{QoS} (\textit{Quality of Service}, Calidad de Servicio).

Para la securización de la red se implantan \textit{firewalls}, \textit{proxys}, e \textit{IDS}s (\textit{Intrusion Detection System}, Sistema de Detección de Intrusión), que se encargan de evitar accesos externos no autorizados a la red, controlar el tráfico saliente, y detectar posibles atacantes dentro de la red.

El método para asegurar la calidad de las comunicaciones es la medición de parámetros de \textit{QoS}, tales como retardo de los paquetes, errores en la transmisión, tipos de tráfico comunes, rutas óptimas, etc.

Son varias las soluciones comerciales que implementan estos servicios pero actualmente no hay ninguna que implemente todo en un mismo sistema. Además, no hay ningún producto de código libre que aproveche al máximo la posible eficiencia del equipo, haciendo un análisis \textit{online}\footnote{Online se refiere a hacer el análisis mientras se captura, en vez de capturar, guardar y luego analizar.}.

En el grupo de investigación \textit{Network Quality and Security (NQAS)} se ha diseñado un sonda que implementa algunas de las citadas funcionalidades. En el diseño de la misma se ha contemplado la posibilidad de más adelante poder ampliarlo de una manera sencilla. Como valor añadido fundamental, aprovecha al máximo los recursos disponibles, ya que ha sido programado de una manera que permita ejecución multi-hilo.

Este prototipo se llama ksensor\cite{KABO05}, aunque no es el primero que se ha creado. La línea de investigación principal, llamada hi-sensor, ha ido haciendo cada vez más complejo el diseño y la programación del prototipo con el objetivo de mejorar la eficacia.

A grandes rasgos, primero se diseñó de forma que la aplicación fuera un programa de usuario que capturaba a través de una librería estándar para la captura de paquetes de forma promiscua. Se utilizaron también las librerías para tener computación multi-hilo, y se consiguieron unos resultados prometedores.

Cuando se empezaron a analizar redes de alta velocidad en saturación se vio que había un problema: El equipo estaba continuamente capturando, y no había espacio para el análisis. Para solucionar este problema, se migró a espacio de kernel.

El kernel es la base que hace que todos los programas funcionen. Es un programa que se encarga de gestionar los dispositivos y recursos del ordenador para ofrecer una interfaz libre de detalles a los programas. Además, también se encarga de hacer que varios programas se ejecuten de una manera que proporcione a todos y cada uno la forma de parecer que están en ejecución continua. A parte de eso, se encarga de que los programas sean capaces de hacer un direccionamiento virtual de una forma transparente a ellos, manejando eficazmente una MMU (\textit{Memory Management Unit}, Unidad de Gestión de Memoria), de manera que permite que estos sea posible compilarlos como si solo fueran a existir ellos.

En general, del diseño de un buen kernel dependerá la eficiencia del sistema. En nuestro caso, el diseño del kernel no está hecho para nuestro tipo de sistema, una sonda de tráfico, pero a través de una serie de modificaciones, podemos cambiar el comportamiento del sistema para amoldarlo a nuestro caso de uso. Todo sea dicho, la eficiencia del kernel actual es buena para los sistemas operativos de carácter general, en los que capturar los paquetes de uno mismo es suficiente, y que es una cantidad pequeña en comparación con el tráfico de la red.

El cometido del sistema que queremos es únicamente para capturar y procesar paquetes y por lo tanto, la máxima eficiencia se puede describir como el equilibrio entre el tiempo en el que el ordenador esta capturando y el que está analizando. Se tienen que capturar exactamente el número de paquetes que se van a analizar.

La implementación actual del sistema ha quedado obsoleta para kernels actuales, ya que el tratamiento de los paquetes se hace de una forma diferente. Ahora se crean \textit{superpaquetes} ensamblando desde la captura los paquetes que tienen una serie de parámetros iguales, como pueden ser la IP de origen, IP de destino, puerto TCP destino y origen. De esta manera, al pasar un único paquete (aunque grande) a la pila de protocolos, se acelera su tratamiento, ya que las copias de los paquetes se pueden hacer en una única transacción.

Por lo tanto, el proyecto se enfoca a mejorar el anterior diseño para adaptarlo a la nueva manera de hacer las cosas, crear una serie de herramientas para validar el diseño y adaptar el proyecto a los estándares defacto existentes en kernel.org para hacer posible la liberación pública del código. La tarea de mayor relevancia en estos momentos es posibilitar un estudio comparativo entre un sistema normal y la aplicación.